{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19845013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80b01a",
   "metadata": {},
   "source": [
    "# Downloading and preprocessing MNIST ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fff0ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Преобразования для нормализации изображений\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))]) \n",
    "\n",
    "# Загрузка обучающего и тестового наборов данных MNIST\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea3667",
   "metadata": {},
   "source": [
    "Тут возник вопрос, как праильно вычислять mean и std:\n",
    "По всему data set'у или в каждом изображении/батче отдельно?\n",
    "\n",
    "значения 0.5 и 0.5 были взяты из интернета, когда я гуглил про то, как скачать mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ff8ca32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size: 60000 \n",
      " Test data set size: 10000\n"
     ]
    }
   ],
   "source": [
    "print('Train data set size:', len(train_dataset),'\\n', 'Test data set size:', len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538973c",
   "metadata": {},
   "source": [
    "## Making a Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a38a8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f34ef6",
   "metadata": {},
   "source": [
    "## Transfer to CUDA if it is available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0182d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device for running and trainning: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Current device for running and trainning:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a61071",
   "metadata": {},
   "source": [
    "Я юы хотел перенести dataset или dataloader сразу все данные, но попытки были безуспешны.\n",
    "\n",
    "Как можно перенести данные на cuda до начала цикла обучения ?\n",
    "\n",
    "там не так много данных и я могу себе позволить перенести весь датасет на видеокарту "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45216d1e",
   "metadata": {},
   "source": [
    "я не знаю как inplace изменить dataset или dataloader что бы применить one hot и labels. Придется каждый раз считать это в цикле обучения("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25273a6e",
   "metadata": {},
   "source": [
    "# Model creating and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6979521c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9f347",
   "metadata": {},
   "source": [
    "входное изображение 28х28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4592011b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_img, label = train_dataset[0]\n",
    "test_img = test_img.squeeze()  # Убираем размерность канала, если она равна 1\n",
    "\n",
    "# Вывод изображения с помощью Matplotlib\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "plt.title(f\"Digit: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf119ccc",
   "metadata": {},
   "source": [
    "обучающая функция\n",
    "\n",
    "вынес ее отдельно тк она общая для каждой модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69d72cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_size = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            \n",
    "            total_size += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "    #print(\"Accuracy:\", correct / total_size)\n",
    "    #print('True predicted', correct, '/', total_size)\n",
    "        return correct, total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "302ead03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "num_classes = 10 # from 0 to 9\n",
    "\n",
    "from TgReporter import send_learning_results\n",
    "\n",
    "#labels = torch.empty(batch_size, num_classes, )\n",
    "#labels = labels.to(device)\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    log_records = []\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader: \n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = torch.eye(num_classes).to(device)[labels]#.to(device)\n",
    "            \n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            log_rec1 = '\\n{}\\nEpoch {}\\nTraining loss {}\\n'.format(\n",
    "                datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), epoch,\n",
    "                loss_train / len(train_loader))\n",
    "\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), epoch,\n",
    "                loss_train / len(train_loader)))\n",
    "            correct, total_size = evaluate_model(model, test_loader)\n",
    "\n",
    "            log_rec2 = \"Accuracy:\" + str(correct / total_size) + '\\n'\n",
    "            print(log_rec2)\n",
    "            log_records.append(log_rec1)\n",
    "            log_records.append(log_rec2)\n",
    "    #\n",
    "    try:\n",
    "        send_learning_results(log = log_records)\n",
    "    except:\n",
    "        print('Cant, send the result')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9d87a",
   "metadata": {},
   "source": [
    "## Ordinary model\n",
    "попробуем без Dropout, Batchnorm и Skipconnection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c0c76f",
   "metadata": {},
   "source": [
    "этот кусок кода я скопировал из книжки и немного дополнил - one hot encoding для lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfa14d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 15:29:28 Epoch 1, Training loss 1.3840321097006676\n",
      "\tAccuracy:0.8952\n",
      "2023-08-26 15:31:24 Epoch 10, Training loss 0.07019082616879614\n",
      "\tAccuracy:0.9787\n",
      "2023-08-26 15:33:35 Epoch 20, Training loss 0.04452780285191887\n",
      "\tAccuracy:0.9834\n",
      "2023-08-26 15:35:47 Epoch 30, Training loss 0.03329354745317509\n",
      "\tAccuracy:0.9861\n",
      "2023-08-26 15:38:01 Epoch 40, Training loss 0.026040701806716034\n",
      "\tAccuracy:0.9865\n",
      "2023-08-26 15:40:16 Epoch 50, Training loss 0.020964564027249572\n",
      "\tAccuracy:0.9872\n",
      "2023-08-26 15:42:27 Epoch 60, Training loss 0.017093962960612667\n",
      "\tAccuracy:0.9851\n",
      "2023-08-26 15:44:37 Epoch 70, Training loss 0.013861957982973147\n",
      "\tAccuracy:0.9879\n",
      "2023-08-26 15:46:52 Epoch 80, Training loss 0.011015284153674428\n",
      "\tAccuracy:0.9858\n",
      "2023-08-26 15:49:00 Epoch 90, Training loss 0.009189979076925812\n",
      "\tAccuracy:0.987\n",
      "2023-08-26 15:51:10 Epoch 100, Training loss 0.007476140212519604\n",
      "\tAccuracy:0.9868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9868, 10000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_out_ch = 16\n",
    "conv2_out_ch = 8\n",
    "kernel_size = 3\n",
    "\n",
    "ordinary_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(8 * 7 * 7, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "ordinary_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(ordinary_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, ordinary_model, loss_fn, train_loader)\n",
    "evaluate_model(ordinary_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fa93ce8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9861\n",
      "True predicted 9861 / 10000\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(ordinary_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35285a3c",
   "metadata": {},
   "source": [
    "Даже обычная сверточная модел без Dropout, Batchnorm и Skipconnection справилась на этом датасете неплохо(хотя и датасет простой)\n",
    "\n",
    "1st Test:  \n",
    "Точность: 98.74%  \n",
    "Праильно: 9874 / 10_000  \n",
    "\n",
    "2nd Test:  \n",
    "Точность: 98.61%  \n",
    "Праильно: 9861 / 10_000  \n",
    "из-за своего косяка мне пришлось дважды обучить модель, поэтому тут два теста\n",
    "\n",
    "3rd Test:\n",
    "Точность: 98.89%  \n",
    "Праильно: 9898 / 10_000  \n",
    "лучший результат достигается в самом конце"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6471c0",
   "metadata": {},
   "source": [
    "Тут возникает вопрос, стоит ли улучшать модель, тк улучшения могут быть и не особо заметны, но почему бы и нет "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dced409",
   "metadata": {},
   "source": [
    "## Ordinary model + Tanh\n",
    "Хочу протестировать с другой функцией активации \n",
    "\n",
    "Заменим все ReLU на Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7a44f856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-25 18:04:31 Epoch 1, Training loss 1.3120998711601248\n",
      "\tAccuracy: 0.8668\n",
      "2023-08-25 18:06:19 Epoch 10, Training loss 0.11606150145517356\n",
      "\tAccuracy: 0.9707\n",
      "2023-08-25 18:08:23 Epoch 20, Training loss 0.0729654192987051\n",
      "\tAccuracy: 0.979\n",
      "2023-08-25 18:10:22 Epoch 30, Training loss 0.05647478429382163\n",
      "\tAccuracy: 0.9829\n",
      "2023-08-25 18:12:22 Epoch 40, Training loss 0.04732624837296651\n",
      "\tAccuracy: 0.9843\n",
      "2023-08-25 18:14:26 Epoch 50, Training loss 0.040922385865329966\n",
      "\tAccuracy: 0.9853\n",
      "2023-08-25 18:16:25 Epoch 60, Training loss 0.03634378703873056\n",
      "\tAccuracy: 0.986\n",
      "2023-08-25 18:18:26 Epoch 70, Training loss 0.032778651035551644\n",
      "\tAccuracy: 0.9869\n",
      "2023-08-25 18:20:29 Epoch 80, Training loss 0.02968440427959188\n",
      "\tAccuracy: 0.9876\n",
      "2023-08-25 18:22:28 Epoch 90, Training loss 0.027158318939474797\n",
      "\tAccuracy: 0.9868\n",
      "2023-08-25 18:24:29 Epoch 100, Training loss 0.024912030065629774\n",
      "\tAccuracy: 0.9872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9872, 10000)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_out_ch = 16\n",
    "conv2_out_ch = 8\n",
    "kernel_size = 3\n",
    "\n",
    "model_tanh = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(8 * 7 * 7, 32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_tanh.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model_tanh.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_tanh, loss_fn, train_loader)\n",
    "\n",
    "evaluate_model(model_tanh, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b396aa3e",
   "metadata": {},
   "source": [
    "Ну чет разницы вообще почти нет, на 2 стоые лучше чем у предыдущей модели те текущая модель предсказала правильно на 2 картики больше \n",
    "\n",
    "1st Test:  \n",
    "Точность: 98.76%  \n",
    "Праильно: 9876 / 10_000  \n",
    "\n",
    "2nd Test:  \n",
    "Точность: 98.72%  \n",
    "Праильно: 9872 / 10_000  \n",
    "после 80ой эпохи результат на 4 сотые процента лучше, скорее погрешность \n",
    "\n",
    "Так же я ожидал что она будет работать дольше, тк слышал из лекций, но и чисто логически понятно, что производная ReLU намного проще считается. \n",
    "Однако тут все же есть разница ~10 секунд дольше иногда для 10 эпох"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80225596",
   "metadata": {},
   "source": [
    "## Ordinary model + Adam\n",
    "Заменим SGD на Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a606087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-25 18:24:45 Epoch 1, Training loss 0.18735437904959168\n",
      "\tAccuracy: 0.9722\n",
      "2023-08-25 18:26:35 Epoch 10, Training loss 0.07436344940163223\n",
      "\tAccuracy: 0.9779\n",
      "2023-08-25 18:28:35 Epoch 20, Training loss 0.06704386312204019\n",
      "\tAccuracy: 0.9756\n",
      "2023-08-25 18:30:38 Epoch 30, Training loss 0.06595333824544819\n",
      "\tAccuracy: 0.9708\n",
      "2023-08-25 18:32:44 Epoch 40, Training loss 0.06569145562139396\n",
      "\tAccuracy: 0.975\n",
      "2023-08-25 18:34:43 Epoch 50, Training loss 0.06226704582299686\n",
      "\tAccuracy: 0.9761\n",
      "2023-08-25 18:36:47 Epoch 60, Training loss 0.060985493739583604\n",
      "\tAccuracy: 0.9752\n",
      "2023-08-25 18:38:51 Epoch 70, Training loss 0.062163011309166905\n",
      "\tAccuracy: 0.978\n",
      "2023-08-25 18:40:51 Epoch 80, Training loss 0.057178736665409735\n",
      "\tAccuracy: 0.9773\n",
      "2023-08-25 18:42:56 Epoch 90, Training loss 0.06171750899887138\n",
      "\tAccuracy: 0.9758\n",
      "2023-08-25 18:44:59 Epoch 100, Training loss 0.06370039359106484\n",
      "\tAccuracy: 0.9727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9727, 10000)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_out_ch = 16\n",
    "conv2_out_ch = 8\n",
    "kernel_size = 3\n",
    "\n",
    "adam_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(8 * 7 * 7, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "adam_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.Adam(adam_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, adam_model, loss_fn, train_loader)\n",
    "\n",
    "\n",
    "evaluate_model(adam_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d42aa",
   "metadata": {},
   "source": [
    "Adam сразу начал с низким Training loss'ом по сравнению с предыдущими моделями\n",
    "\n",
    "Возможно на 40й эпохе нужно остановить. Нужно каждые 10 эпох проверять на тестовом датасете и выводить данные, проверка занимает не много времени. Обидно, как я раньше не додумался - теперь нужно будет перезапускачть все модели, что бы посмотреть на изменения точности на тесте. Для этого нужно дополнить функцию evaluate_model\n",
    "\n",
    "1st Test:  \n",
    "Точность: 97.43%  \n",
    "Праильно: 9743 / 10_000    \n",
    "Точность хуже, чем у предыдущих примерно на процент те на 100 неверных ответов больше \n",
    "\n",
    "2тв Test:  \n",
    "Точность: 97.27%  \n",
    "Праильно: 9727 / 10_000  \n",
    "Как я и предполагал тут быстро происходит насыщение. и после 70ой эпохи точность падает на 6 десятых процента"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb39c6",
   "metadata": {},
   "source": [
    "## Ordinary model + DropOut\n",
    "Добавим DropOut в обычную модель (с ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ef36169f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-25 18:45:14 Epoch 1, Training loss 1.3749489463023794\n",
      "\tAccuracy: 0.8773\n",
      "2023-08-25 18:47:01 Epoch 10, Training loss 0.0749382493840749\n",
      "\tAccuracy: 0.9788\n",
      "2023-08-25 18:49:01 Epoch 20, Training loss 0.046964089866223205\n",
      "\tAccuracy: 0.9822\n",
      "2023-08-25 18:51:01 Epoch 30, Training loss 0.03534409180297199\n",
      "\tAccuracy: 0.9851\n",
      "2023-08-25 18:52:59 Epoch 40, Training loss 0.028374327570825482\n",
      "\tAccuracy: 0.9867\n",
      "2023-08-25 18:54:58 Epoch 50, Training loss 0.02277248651998714\n",
      "\tAccuracy: 0.9875\n",
      "2023-08-25 18:56:58 Epoch 60, Training loss 0.018946958165965786\n",
      "\tAccuracy: 0.9868\n",
      "2023-08-25 18:58:55 Epoch 70, Training loss 0.016052112233962978\n",
      "\tAccuracy: 0.9871\n",
      "2023-08-25 19:00:55 Epoch 80, Training loss 0.013304501076941061\n",
      "\tAccuracy: 0.9869\n",
      "2023-08-25 19:02:55 Epoch 90, Training loss 0.011527001280668636\n",
      "\tAccuracy: 0.9868\n",
      "2023-08-25 19:04:53 Epoch 100, Training loss 0.009874266363403722\n",
      "\tAccuracy: 0.9864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9864, 10000)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_out_ch = 16\n",
    "conv2_out_ch = 8\n",
    "kernel_size = 3\n",
    "\n",
    "dropout_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Dropout(p=0.3), # 1st dropout\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Dropout(p=0.3), # 2nd dropout\n",
    "\n",
    "            nn.Linear(8 * 7 * 7, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "dropout_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(dropout_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, dropout_model, loss_fn, train_loader)\n",
    "\n",
    "\n",
    "evaluate_model(dropout_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38e194",
   "metadata": {},
   "source": [
    "С р=0.3 результат не особо лучше обычной модели, возможно имеет смысл попробовать р побольше, но я боюсь, что из за наложения на начальных слоях останется мало нейронов\n",
    "\n",
    "\n",
    "1st Test:  \n",
    "Точность: 98.64%  \n",
    "Праильно: 9764 / 10_000  \n",
    "Тоже относительно быстро происходит насыщение лучший результат - 98.75% на 50ой эпохе "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3567d",
   "metadata": {},
   "source": [
    "## Ordinary model + BatchNorm\n",
    "Добавим BatchNorm в обычную модель (с ReLU)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d74166e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-25 19:05:09 Epoch 1, Training loss 0.389585454191671\n",
      "\tAccuracy: 0.9738\n",
      "2023-08-25 19:07:01 Epoch 10, Training loss 0.027030378143566954\n",
      "\tAccuracy: 0.9853\n",
      "2023-08-25 19:09:05 Epoch 20, Training loss 0.012746241657109721\n",
      "\tAccuracy: 0.989\n",
      "2023-08-25 19:11:07 Epoch 30, Training loss 0.005298139848686593\n",
      "\tAccuracy: 0.9875\n",
      "2023-08-25 19:13:12 Epoch 40, Training loss 0.0020841108131474033\n",
      "\tAccuracy: 0.9881\n",
      "2023-08-25 19:15:15 Epoch 50, Training loss 0.0005142504325329684\n",
      "\tAccuracy: 0.9885\n",
      "2023-08-25 19:17:17 Epoch 60, Training loss 0.00023568552932081774\n",
      "\tAccuracy: 0.9881\n",
      "2023-08-25 19:19:22 Epoch 70, Training loss 0.0001606414651027439\n",
      "\tAccuracy: 0.9881\n",
      "2023-08-25 19:21:25 Epoch 80, Training loss 0.00012169297149120584\n",
      "\tAccuracy: 0.9882\n",
      "2023-08-25 19:23:29 Epoch 90, Training loss 9.596177190213616e-05\n",
      "\tAccuracy: 0.9882\n",
      "2023-08-25 19:25:35 Epoch 100, Training loss 7.95941465593971e-05\n",
      "\tAccuracy: 0.9881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9881, 10000)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1_out_ch = 16\n",
    "conv2_out_ch = 8\n",
    "kernel_size = 3\n",
    "\n",
    "batchnorm_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.BatchNorm2d( conv1_out_ch ), # 1st BatchNorm\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.BatchNorm2d( conv2_out_ch ), # 2nd BatchNorm\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(8 * 7 * 7, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(32),\n",
    "\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "batchnorm_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(batchnorm_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, batchnorm_model, loss_fn, train_loader)\n",
    "\n",
    "\n",
    "evaluate_model(batchnorm_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa1d74",
   "metadata": {},
   "source": [
    "Данная модель, наверное, показала самый лучший результат за обучение на 1ой эпохе\n",
    "1st Test:  \n",
    "Точность: 98.81%  \n",
    "Праильно: 9781 / 10_000  \n",
    "Лучший результат - 98.9 на 20ой эпохе "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007a12f",
   "metadata": {},
   "source": [
    "## Ordinary model + DropOut с р=0.5\n",
    "Добавим DropOut в обычную модель (с ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc1e459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 10:29:56 Epoch 1, Training loss 1.6380273519929793\n",
      "\tAccuracy: 0.8326\n",
      "2023-08-26 10:31:47 Epoch 10, Training loss 0.07574204474028304\n",
      "\tAccuracy: 0.9755\n",
      "2023-08-26 10:33:47 Epoch 20, Training loss 0.0467621037948267\n",
      "\tAccuracy: 0.9857\n",
      "2023-08-26 10:35:51 Epoch 30, Training loss 0.034718165360167745\n",
      "\tAccuracy: 0.9844\n",
      "2023-08-26 10:37:52 Epoch 40, Training loss 0.026983658605784392\n",
      "\tAccuracy: 0.9873\n",
      "2023-08-26 10:39:52 Epoch 50, Training loss 0.022238938759835902\n",
      "\tAccuracy: 0.9873\n",
      "2023-08-26 10:41:55 Epoch 60, Training loss 0.017940459963748295\n",
      "\tAccuracy: 0.987\n",
      "2023-08-26 10:43:55 Epoch 70, Training loss 0.014985951985389098\n",
      "\tAccuracy: 0.9884\n",
      "2023-08-26 10:45:54 Epoch 80, Training loss 0.012904158337668958\n",
      "\tAccuracy: 0.9884\n",
      "2023-08-26 10:47:53 Epoch 90, Training loss 0.010349543445444608\n",
      "\tAccuracy: 0.9887\n",
      "2023-08-26 10:49:52 Epoch 100, Training loss 0.009093142805283696\n",
      "\tAccuracy: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9893, 10000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conv1_out_ch = 16\n",
    "conv2_out_ch = 8\n",
    "kernel_size = 3\n",
    "\n",
    "dropout05_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Dropout(p=0.5), # 1st dropout\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Dropout(p=0.5), # 2nd dropout\n",
    "\n",
    "            nn.Linear(8 * 7 * 7, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "dropout05_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(dropout05_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, dropout05_model, loss_fn, train_loader)\n",
    "\n",
    "\n",
    "evaluate_model(dropout05_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cba746",
   "metadata": {},
   "source": [
    "Стало лучше на 2 десятых процента\n",
    "\n",
    "1st Test:  \n",
    "Точность: 98.85%  \n",
    "Лучгий результат - 98.88 на 90ой эпохе\n",
    "\n",
    "2nd Test:  \n",
    "Точность: 98.93%  \n",
    "Случайно запуститл второй раз, но вроде лучший результат по сравнению со всеми"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c3835",
   "metadata": {},
   "source": [
    "## Ordinary model + DropOut(р=0.5) + BatchNorm\n",
    "Добавим DropOut и BatchNorm в обычную модель (с ReLU)  \n",
    "Batchnorm после Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c621887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 11:00:07 Epoch 1, Training loss 1.0098676786684533\n",
      "\tAccuracy: 0.9345\n",
      "2023-08-26 11:02:02 Epoch 10, Training loss 0.039496858333195765\n",
      "\tAccuracy: 0.9862\n",
      "2023-08-26 11:04:05 Epoch 20, Training loss 0.023167732430421917\n",
      "\tAccuracy: 0.9881\n",
      "2023-08-26 11:06:16 Epoch 30, Training loss 0.014965355469565553\n",
      "\tAccuracy: 0.9878\n",
      "2023-08-26 11:08:21 Epoch 40, Training loss 0.009213419598856843\n",
      "\tAccuracy: 0.9867\n",
      "2023-08-26 11:10:25 Epoch 50, Training loss 0.0053512684803109474\n",
      "\tAccuracy: 0.9881\n",
      "2023-08-26 11:12:30 Epoch 60, Training loss 0.0033945000688075425\n",
      "\tAccuracy: 0.989\n",
      "2023-08-26 11:14:31 Epoch 70, Training loss 0.001581393994010791\n",
      "\tAccuracy: 0.9887\n",
      "2023-08-26 11:16:31 Epoch 80, Training loss 0.0007032315937035115\n",
      "\tAccuracy: 0.9885\n",
      "2023-08-26 11:18:32 Epoch 90, Training loss 0.0004270266325524881\n",
      "\tAccuracy: 0.9887\n",
      "2023-08-26 11:20:32 Epoch 100, Training loss 0.00027878667411717045\n",
      "\tAccuracy: 0.9885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9885, 10000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conv1_out_ch = 16\n",
    "conv2_out_ch = 8\n",
    "kernel_size = 3\n",
    "\n",
    "d_b_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Dropout(p=0.5), # 1st \n",
    "            nn.BatchNorm2d( conv1_out_ch ), # 1st BatchNorm\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Dropout(p=0.5), # 2nd \n",
    "            nn.BatchNorm2d( conv2_out_ch ), # 2nd BatchNorm\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(8 * 7 * 7, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.BatchNorm1d(32), # 3rd batchnorm\n",
    "\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "d_b_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(d_b_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, d_b_model, loss_fn, train_loader)\n",
    "\n",
    "\n",
    "evaluate_model(d_b_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d1862",
   "metadata": {},
   "source": [
    "Ну ничего особенного примерно те же значения, что и у остальных, примерно то что я и ожидал, хотя ,конечно, хотелось бы 99%   \n",
    "Я пологая, что так произошло из-за недостаточной глубины/размерноти слоев.\n",
    "\n",
    "1st Test:  \n",
    "Точность: 98.85%  \n",
    "Лучший результат - 98.87 на 90ой эпохе"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c002c8",
   "metadata": {},
   "source": [
    "## Wider conv layers Ordinary model\n",
    "повысим размерность сверточных слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1d1669c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 11:39:33 Epoch 1, Training loss 1.0107261456056698\n",
      "\tAccuracy: 0.8973\n",
      "2023-08-26 11:41:32 Epoch 10, Training loss 0.05805535578498565\n",
      "\tAccuracy: 0.9837\n",
      "2023-08-26 11:43:40 Epoch 20, Training loss 0.036632246226752437\n",
      "\tAccuracy: 0.9877\n",
      "2023-08-26 11:45:43 Epoch 30, Training loss 0.026469037087677894\n",
      "\tAccuracy: 0.9891\n",
      "2023-08-26 11:47:57 Epoch 40, Training loss 0.019431465918740343\n",
      "\tAccuracy: 0.9886\n",
      "2023-08-26 11:50:02 Epoch 50, Training loss 0.014386922821417071\n",
      "\tAccuracy: 0.9905\n",
      "2023-08-26 11:52:05 Epoch 60, Training loss 0.011258132575525168\n",
      "\tAccuracy: 0.9846\n",
      "2023-08-26 11:54:19 Epoch 70, Training loss 0.008245408549893002\n",
      "\tAccuracy: 0.99\n",
      "2023-08-26 11:56:26 Epoch 80, Training loss 0.006186401398528556\n",
      "\tAccuracy: 0.9898\n",
      "2023-08-26 11:58:41 Epoch 90, Training loss 0.004697458321879143\n",
      "\tAccuracy: 0.99\n",
      "2023-08-26 12:01:01 Epoch 100, Training loss 0.0036937927976698685\n",
      "\tAccuracy: 0.9897\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 32\n",
    "conv2_out_ch = 16\n",
    "kernel_size = 3\n",
    "\n",
    "wide_conv_ordinary_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "wide_conv_ordinary_model.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(wide_conv_ordinary_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, wide_conv_ordinary_model, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd369249",
   "metadata": {},
   "source": [
    "ЕЕЕЕЕ уху 99%  \n",
    "Как я и думал нужно было \"расширить\" слои\n",
    "\n",
    "Надо было остановиться на 50ой эпохе\n",
    "\n",
    "1st Test:  \n",
    "Точность: 98.97%\n",
    "Best: 99.05% на 50й эпохе "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22121a5",
   "metadata": {},
   "source": [
    "## Wider conv layers Ordinary model + lr = 1e-3\n",
    "повысим размерность сверточных слоев   \n",
    "+ я хотел проверить как скажется уменьшение lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d1ab3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 12:04:38 Epoch 1, Training loss 2.2857591300122517\n",
      "\tAccuracy: 0.2596\n",
      "2023-08-26 12:06:47 Epoch 10, Training loss 0.3049766073333048\n",
      "\tAccuracy: 0.9214\n",
      "2023-08-26 12:08:53 Epoch 20, Training loss 0.1730422174284008\n",
      "\tAccuracy: 0.9549\n",
      "2023-08-26 12:11:11 Epoch 30, Training loss 0.12165618759554142\n",
      "\tAccuracy: 0.9682\n",
      "2023-08-26 12:13:25 Epoch 40, Training loss 0.09704615961490219\n",
      "\tAccuracy: 0.9737\n",
      "2023-08-26 12:15:31 Epoch 50, Training loss 0.08276069538890203\n",
      "\tAccuracy: 0.9771\n",
      "2023-08-26 12:17:57 Epoch 60, Training loss 0.07300627716570728\n",
      "\tAccuracy: 0.9797\n",
      "2023-08-26 12:20:06 Epoch 70, Training loss 0.0654491131384569\n",
      "\tAccuracy: 0.9806\n",
      "2023-08-26 12:22:13 Epoch 80, Training loss 0.05991656813017929\n",
      "\tAccuracy: 0.981\n",
      "2023-08-26 12:24:39 Epoch 90, Training loss 0.05498122352796958\n",
      "\tAccuracy: 0.9816\n",
      "2023-08-26 12:26:44 Epoch 100, Training loss 0.05090208112551118\n",
      "\tAccuracy: 0.9834\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 32\n",
    "conv2_out_ch = 16\n",
    "kernel_size = 3\n",
    "\n",
    "wide_conv_lr_ordinary_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "wide_conv_lr_ordinary_model.to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = optim.SGD(wide_conv_lr_ordinary_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, wide_conv_lr_ordinary_model, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7983a",
   "metadata": {},
   "source": [
    "ну чет не особо \n",
    "Теперь попробую увеличить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "36f1d790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 12:36:30 Epoch 1, Training loss 0.27649686131169643\n",
      "\tAccuracy: 0.9758\n",
      "2023-08-26 12:38:22 Epoch 10, Training loss 0.016612038092631506\n",
      "\tAccuracy: 0.9844\n",
      "2023-08-26 12:40:33 Epoch 20, Training loss 0.005398999979600226\n",
      "\tAccuracy: 0.9908\n",
      "2023-08-26 12:42:54 Epoch 30, Training loss 0.0006353820371314669\n",
      "\tAccuracy: 0.9914\n",
      "2023-08-26 12:45:01 Epoch 40, Training loss 0.00013546831767844277\n",
      "\tAccuracy: 0.9912\n",
      "2023-08-26 12:47:20 Epoch 50, Training loss 8.133482572646335e-05\n",
      "\tAccuracy: 0.991\n",
      "2023-08-26 12:49:32 Epoch 60, Training loss 5.931050342691132e-05\n",
      "\tAccuracy: 0.9908\n",
      "2023-08-26 12:51:38 Epoch 70, Training loss 4.5838559885806646e-05\n",
      "\tAccuracy: 0.9907\n",
      "2023-08-26 12:53:51 Epoch 80, Training loss 3.711113531807196e-05\n",
      "\tAccuracy: 0.9908\n",
      "2023-08-26 12:55:59 Epoch 90, Training loss 3.1064888310348855e-05\n",
      "\tAccuracy: 0.9908\n",
      "2023-08-26 12:58:09 Epoch 100, Training loss 2.6732946296808675e-05\n",
      "\tAccuracy: 0.9909\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 32\n",
    "conv2_out_ch = 16\n",
    "kernel_size = 3\n",
    "\n",
    "wide_conv_lr_ordinary_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "wide_conv_lr_ordinary_model.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(wide_conv_lr_ordinary_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, wide_conv_lr_ordinary_model, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752dd8da",
   "metadata": {},
   "source": [
    "Получилось очень хорошо, я даже не ожидал \n",
    "\n",
    "теперь понизим lr еще на степень 10  \n",
    "с lr=1 получилось очень плохо даже код оставлять не буду   \n",
    "Но добавлю линейный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e43d12ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 13:04:26 Epoch 1, Training loss 0.34054480788351565\n",
      "\tAccuracy: 0.9767\n",
      "2023-08-26 13:06:34 Epoch 10, Training loss 0.011747342878796089\n",
      "\tAccuracy: 0.9903\n",
      "2023-08-26 13:08:41 Epoch 20, Training loss 0.0015831269297298383\n",
      "\tAccuracy: 0.991\n",
      "2023-08-26 13:10:56 Epoch 30, Training loss 9.184142955165057e-05\n",
      "\tAccuracy: 0.9917\n",
      "2023-08-26 13:13:10 Epoch 40, Training loss 4.750197762293335e-05\n",
      "\tAccuracy: 0.9919\n",
      "2023-08-26 13:15:16 Epoch 50, Training loss 3.1252571074842686e-05\n",
      "\tAccuracy: 0.9918\n",
      "2023-08-26 13:17:32 Epoch 60, Training loss 2.3121484611579396e-05\n",
      "\tAccuracy: 0.9918\n",
      "2023-08-26 13:19:40 Epoch 70, Training loss 1.8308243573243475e-05\n",
      "\tAccuracy: 0.9918\n",
      "2023-08-26 13:21:48 Epoch 80, Training loss 1.5136188604218176e-05\n",
      "\tAccuracy: 0.9918\n",
      "2023-08-26 13:24:03 Epoch 90, Training loss 1.2819438326595318e-05\n",
      "\tAccuracy: 0.9917\n",
      "2023-08-26 13:26:08 Epoch 100, Training loss 1.1068048960773962e-05\n",
      "\tAccuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 32\n",
    "conv2_out_ch = 16\n",
    "kernel_size = 3\n",
    "\n",
    "model_1 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_1.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_1.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_1, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f4d938",
   "metadata": {},
   "source": [
    "Хороший результат, почти везде 99% +  \n",
    "\n",
    "1st Test:  \n",
    "Точность: 99.18%\n",
    "Best: 99.19% на 40й эпохе "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4dcdb",
   "metadata": {},
   "source": [
    "Попробую с другим lr (was 1e-1/ now 1e-2) \n",
    "+добавил уведомления в tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c48a1862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 15:04:48 Epoch 1, Training loss 1.4951177743960544\n",
      "\tAccuracy:0.8562\n",
      "2023-08-26 15:06:45 Epoch 10, Training loss 0.057379106373917355\n",
      "\tAccuracy:0.9812\n",
      "2023-08-26 15:08:56 Epoch 20, Training loss 0.03071607626750688\n",
      "\tAccuracy:0.9855\n",
      "2023-08-26 15:11:15 Epoch 30, Training loss 0.01938725820550673\n",
      "\tAccuracy:0.9888\n",
      "2023-08-26 15:13:23 Epoch 40, Training loss 0.011992717817464077\n",
      "\tAccuracy:0.9896\n",
      "2023-08-26 15:15:37 Epoch 50, Training loss 0.007712396837367023\n",
      "\tAccuracy:0.9894\n",
      "2023-08-26 15:17:37 Epoch 60, Training loss 0.00487380933645715\n",
      "\tAccuracy:0.9884\n",
      "2023-08-26 15:19:38 Epoch 70, Training loss 0.002874753110681108\n",
      "\tAccuracy:0.9891\n",
      "2023-08-26 15:21:40 Epoch 80, Training loss 0.0020957217874670443\n",
      "\tAccuracy:0.9883\n",
      "2023-08-26 15:23:42 Epoch 90, Training loss 0.0011124931918974651\n",
      "\tAccuracy:0.989\n",
      "2023-08-26 15:25:44 Epoch 100, Training loss 0.0008736247336424264\n",
      "\tAccuracy:0.9894\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 32\n",
    "conv2_out_ch = 16\n",
    "kernel_size = 3\n",
    "\n",
    "model_2 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_2.to(device)\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = optim.SGD(model_2.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_2, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36196d46",
   "metadata": {},
   "source": [
    "Результат похуже - оставляем лр = 1е-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8596cfc",
   "metadata": {},
   "source": [
    "Теперь добавим еще один линейный слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3db0404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 17:02:53 Epoch 1, Training loss 0.5741520727347178\n",
      "\tAccuracy:0.9764\n",
      "2023-08-26 17:05:09 Epoch 10, Training loss 0.012803756991981498\n",
      "\tAccuracy:0.9911\n",
      "2023-08-26 17:07:38 Epoch 20, Training loss 0.0030695520578117477\n",
      "\tAccuracy:0.9905\n",
      "2023-08-26 17:10:14 Epoch 30, Training loss 0.00011431614398513097\n",
      "\tAccuracy:0.9915\n",
      "2023-08-26 17:12:40 Epoch 40, Training loss 3.336929594593406e-05\n",
      "\tAccuracy:0.9918\n",
      "2023-08-26 17:15:13 Epoch 50, Training loss 1.9999376535839458e-05\n",
      "\tAccuracy:0.9918\n",
      "2023-08-26 17:17:41 Epoch 60, Training loss 1.4335545297088819e-05\n",
      "\tAccuracy:0.9918\n",
      "2023-08-26 17:20:11 Epoch 70, Training loss 1.0841767161864835e-05\n",
      "\tAccuracy:0.9918\n",
      "2023-08-26 17:22:44 Epoch 80, Training loss 8.956036205864558e-06\n",
      "\tAccuracy:0.9919\n",
      "2023-08-26 17:25:13 Epoch 90, Training loss 7.434653846143495e-06\n",
      "\tAccuracy:0.9919\n",
      "2023-08-26 17:27:49 Epoch 100, Training loss 6.455055273199638e-06\n",
      "\tAccuracy:0.9919\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 32\n",
    "conv2_out_ch = 16\n",
    "kernel_size = 3\n",
    "\n",
    "model_3 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_3.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_3.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_3, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f8a34",
   "metadata": {},
   "source": [
    "Ничего особо не изменилось"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbee5d1",
   "metadata": {},
   "source": [
    "Добавим еще один сверточный слой\n",
    "\n",
    "+как посчитать размерность для первого линейного слоя ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f04dfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 17:49:06 Epoch 1, Training loss 1.0250194809083808\n",
      "\tAccuracy:0.9677\n",
      "2023-08-26 17:51:28 Epoch 10, Training loss 0.025718436880930284\n",
      "\tAccuracy:0.9853\n",
      "2023-08-26 17:53:58 Epoch 20, Training loss 0.011877770131613574\n",
      "\tAccuracy:0.9879\n",
      "2023-08-26 17:56:27 Epoch 30, Training loss 0.007282938270187904\n",
      "\tAccuracy:0.9901\n",
      "2023-08-26 17:59:01 Epoch 40, Training loss 0.00588299916120283\n",
      "\tAccuracy:0.9893\n",
      "2023-08-26 18:01:31 Epoch 50, Training loss 0.002365109740194961\n",
      "\tAccuracy:0.9864\n",
      "2023-08-26 18:04:08 Epoch 60, Training loss 8.505282166277163e-05\n",
      "\tAccuracy:0.9902\n",
      "2023-08-26 18:06:26 Epoch 70, Training loss 3.2980386351594434e-05\n",
      "\tAccuracy:0.9902\n",
      "2023-08-26 18:08:38 Epoch 80, Training loss 1.6062610572099095e-05\n",
      "\tAccuracy:0.9904\n",
      "2023-08-26 18:10:50 Epoch 90, Training loss 1.1267333673051589e-05\n",
      "\tAccuracy:0.9904\n",
      "2023-08-26 18:13:01 Epoch 100, Training loss 8.209344502942362e-06\n",
      "\tAccuracy:0.9905\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 32\n",
    "conv2_out_ch = 16\n",
    "conv3_out_ch = 8\n",
    "kernel_size = 3\n",
    "\n",
    "model_4 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(conv2_out_ch, conv3_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv3_out_ch * 3 * 3, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_4.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_4.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_4, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4073f0",
   "metadata": {},
   "source": [
    "Стало меньше тк входной ветор из за макспулинга стал еще меньше, попробуем это исправиь "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "219a7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 18:36:07 Epoch 1, Training loss 1.047838811210589\n",
      "\tAccuracy:0.92\n",
      "2023-08-26 18:38:29 Epoch 10, Training loss 0.018751384174163888\n",
      "\tAccuracy:0.9889\n",
      "2023-08-26 18:40:59 Epoch 20, Training loss 0.008046414906706881\n",
      "\tAccuracy:0.9911\n",
      "2023-08-26 18:43:27 Epoch 30, Training loss 0.005207747152446181\n",
      "\tAccuracy:0.9893\n",
      "2023-08-26 18:45:55 Epoch 40, Training loss 0.003951341285933022\n",
      "\tAccuracy:0.9904\n",
      "2023-08-26 18:48:24 Epoch 50, Training loss 2.8074604478064916e-05\n",
      "\tAccuracy:0.9914\n",
      "2023-08-26 18:51:00 Epoch 60, Training loss 1.1932249959466628e-05\n",
      "\tAccuracy:0.9913\n",
      "2023-08-26 18:53:28 Epoch 70, Training loss 7.64389783988106e-06\n",
      "\tAccuracy:0.9914\n",
      "2023-08-26 18:56:04 Epoch 80, Training loss 5.612112213160838e-06\n",
      "\tAccuracy:0.9916\n",
      "2023-08-26 18:58:34 Epoch 90, Training loss 4.418294767486649e-06\n",
      "\tAccuracy:0.9916\n",
      "2023-08-26 19:01:02 Epoch 100, Training loss 3.638709350433004e-06\n",
      "\tAccuracy:0.9916\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 32\n",
    "conv2_out_ch = 16\n",
    "conv3_out_ch = 8\n",
    "kernel_size = 3\n",
    "\n",
    "model_5 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(conv2_out_ch, conv3_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            #nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv3_out_ch * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_5.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_5.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_5, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba711ba",
   "metadata": {},
   "source": [
    "стало немного получше, но не сильно  \n",
    "не думаю, что этот слой сильно влияет   \n",
    "\n",
    "Попробую его убрать и увеличить число карт активации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2eadc290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 19:04:19 Epoch 1, Training loss 0.44361017137142356\n",
      "\tAccuracy:0.9726\n",
      "2023-08-26 19:06:44 Epoch 10, Training loss 0.006993106415214278\n",
      "\tAccuracy:0.9913\n",
      "2023-08-26 19:09:23 Epoch 20, Training loss 0.00043088385822222393\n",
      "\tAccuracy:0.9923\n",
      "2023-08-26 19:12:02 Epoch 30, Training loss 3.548040469271936e-05\n",
      "\tAccuracy:0.9925\n",
      "2023-08-26 19:14:43 Epoch 40, Training loss 1.8899088819287472e-05\n",
      "\tAccuracy:0.9925\n",
      "2023-08-26 19:17:23 Epoch 50, Training loss 1.300419809771089e-05\n",
      "\tAccuracy:0.9925\n",
      "2023-08-26 19:20:03 Epoch 60, Training loss 9.918983907977972e-06\n",
      "\tAccuracy:0.9927\n",
      "2023-08-26 19:22:42 Epoch 70, Training loss 7.995720888159677e-06\n",
      "\tAccuracy:0.9926\n",
      "2023-08-26 19:25:21 Epoch 80, Training loss 6.629628871713884e-06\n",
      "\tAccuracy:0.9925\n",
      "2023-08-26 19:28:00 Epoch 90, Training loss 5.667499471524441e-06\n",
      "\tAccuracy:0.9925\n",
      "2023-08-26 19:30:39 Epoch 100, Training loss 4.954208765948891e-06\n",
      "\tAccuracy:0.9924\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 128\n",
    "conv2_out_ch = 64\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "model_6 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_6.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_6.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_6, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbb03c",
   "metadata": {},
   "source": [
    "есть значительное улучшение, но насыщение досигается почти сразу  \n",
    "Время работы сети значительно увеличилось. Предыдущие обучались за ~20 минут, то сейчас ~26\n",
    "\n",
    "1st Test:  \n",
    "Точность: 99.24%\n",
    "Best: 99.27% на 60й эпохе \n",
    "\n",
    "Думаю размерность входново ветора большая для сети и нужно ее увеличить путем добавления линейных слоев и dropout'a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "daf741fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 19:32:45 Epoch 1, Training loss 0.719354201511327\n",
      "\tAccuracy:0.965\n",
      "\n",
      "2023-08-26 19:35:08 Epoch 10, Training loss 0.008070264120812713\n",
      "\tAccuracy:0.9919\n",
      "\n",
      "2023-08-26 19:37:46 Epoch 20, Training loss 0.000498010350774564\n",
      "\tAccuracy:0.9924\n",
      "\n",
      "2023-08-26 19:40:25 Epoch 30, Training loss 2.049967787460156e-05\n",
      "\tAccuracy:0.9927\n",
      "\n",
      "2023-08-26 19:43:03 Epoch 40, Training loss 1.0536446375504065e-05\n",
      "\tAccuracy:0.9928\n",
      "\n",
      "2023-08-26 19:45:42 Epoch 50, Training loss 7.14707752299601e-06\n",
      "\tAccuracy:0.9929\n",
      "\n",
      "2023-08-26 19:48:21 Epoch 60, Training loss 5.353845137064032e-06\n",
      "\tAccuracy:0.9928\n",
      "\n",
      "2023-08-26 19:50:59 Epoch 70, Training loss 4.272594302888849e-06\n",
      "\tAccuracy:0.9927\n",
      "\n",
      "2023-08-26 19:53:38 Epoch 80, Training loss 3.5377128917404613e-06\n",
      "\tAccuracy:0.9927\n",
      "\n",
      "2023-08-26 19:56:17 Epoch 90, Training loss 3.0195301598500237e-06\n",
      "\tAccuracy:0.9928\n",
      "\n",
      "2023-08-26 19:58:56 Epoch 100, Training loss 2.621919798922384e-06\n",
      "\tAccuracy:0.9928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 128\n",
    "conv2_out_ch = 64\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "model_6 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_6.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_6.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_6, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b445cc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "386f59da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 20:00:39 Epoch 1, Training loss 0.16828496586769692\n",
      "\tAccuracy:0.9795\n",
      "\n",
      "2023-08-26 20:03:02 Epoch 10, Training loss nan\n",
      "\tAccuracy:0.098\n",
      "\n",
      "2023-08-26 20:05:42 Epoch 20, Training loss nan\n",
      "\tAccuracy:0.098\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \n\u001b[0;32m     46\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m---> 48\u001b[0m training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)\n",
      "Cell \u001b[1;32mIn[55], line 13\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     12\u001b[0m     loss_train \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mfor\u001b[39;00m imgs, labels \u001b[39min\u001b[39;00m train_loader: \n\u001b[0;32m     14\u001b[0m         imgs \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m         labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mdefault_float_dtype)\u001b[39m.\u001b[39mdiv(\u001b[39m255\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 128\n",
    "conv2_out_ch = 64\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "model_7 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.BatchNorm2d( conv1_out_ch ),\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(128),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_7.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_7.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1995e2",
   "metadata": {},
   "source": [
    "1ая Нормальзация все поламала "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b165795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 20:08:00 Epoch 1, Training loss 0.19431658565148172\n",
      "\tAccuracy:0.9765\n",
      "\n",
      "2023-08-26 20:10:23 Epoch 10, Training loss nan\n",
      "\tAccuracy:0.098\n",
      "\n",
      "2023-08-26 20:13:01 Epoch 20, Training loss nan\n",
      "\tAccuracy:0.098\n",
      "\n",
      "2023-08-26 20:15:40 Epoch 30, Training loss nan\n",
      "\tAccuracy:0.098\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 48\u001b[0m\n\u001b[0;32m     44\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \n\u001b[0;32m     46\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m---> 48\u001b[0m training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)\n",
      "Cell \u001b[1;32mIn[55], line 13\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     12\u001b[0m     loss_train \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mfor\u001b[39;00m imgs, labels \u001b[39min\u001b[39;00m train_loader: \n\u001b[0;32m     14\u001b[0m         imgs \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m         labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mnormalize(tensor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmean, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstd, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minplace)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(tensor, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    361\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mimg should be Tensor Image. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(tensor)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mnormalize(tensor, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd, inplace\u001b[39m=\u001b[39minplace)\n",
      "File \u001b[1;32md:\\Anaconda\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:922\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    920\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(mean, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    921\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(std, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m--> 922\u001b[0m \u001b[39mif\u001b[39;00m (std \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many():\n\u001b[0;32m    923\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstd evaluated to zero after conversion to \u001b[39m\u001b[39m{\u001b[39;00mdtype\u001b[39m}\u001b[39;00m\u001b[39m, leading to division by zero.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    924\u001b[0m \u001b[39mif\u001b[39;00m mean\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 128\n",
    "conv2_out_ch = 64\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "model_7 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            #nn.BatchNorm2d( conv1_out_ch ),\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(128),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_7.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_7.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d12a67",
   "metadata": {},
   "source": [
    "не понимаю почему так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6dcb6011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 20:18:02 Epoch 1, Training loss 0.6251057244634737\n",
      "\tAccuracy:0.9729\n",
      "\n",
      "2023-08-26 20:20:25 Epoch 10, Training loss 0.005165174358302977\n",
      "\tAccuracy:0.992\n",
      "\n",
      "2023-08-26 20:23:05 Epoch 20, Training loss 0.0014821180384100598\n",
      "\tAccuracy:0.9925\n",
      "\n",
      "2023-08-26 20:25:43 Epoch 30, Training loss 2.4638966679781304e-05\n",
      "\tAccuracy:0.9928\n",
      "\n",
      "2023-08-26 20:28:23 Epoch 40, Training loss 1.209994966387474e-05\n",
      "\tAccuracy:0.9929\n",
      "\n",
      "2023-08-26 20:31:01 Epoch 50, Training loss 7.964848485364258e-06\n",
      "\tAccuracy:0.9927\n",
      "\n",
      "2023-08-26 20:33:40 Epoch 60, Training loss 5.8729363963014036e-06\n",
      "\tAccuracy:0.9927\n",
      "\n",
      "2023-08-26 20:36:19 Epoch 70, Training loss 4.6607176097883115e-06\n",
      "\tAccuracy:0.9927\n",
      "\n",
      "2023-08-26 20:38:58 Epoch 80, Training loss 3.834166357358712e-06\n",
      "\tAccuracy:0.9928\n",
      "\n",
      "2023-08-26 20:41:36 Epoch 90, Training loss 3.2460856372028343e-06\n",
      "\tAccuracy:0.9927\n",
      "\n",
      "2023-08-26 20:44:15 Epoch 100, Training loss 2.8039327529625743e-06\n",
      "\tAccuracy:0.9926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 128\n",
    "conv2_out_ch = 128\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "model_7 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_7.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_7.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f11fd9d",
   "metadata": {},
   "source": [
    "Изменеие размера второй свертки не особо повлияло \n",
    "Попробую увеличить число карт активации и добавлю слой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa434105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-26 20:52:02 Epoch 1, Training loss 1.0497215066982835\n",
      "Accuracy:0.9749\n",
      "\n",
      "2023-08-26 20:55:02 Epoch 10, Training loss 0.006723085915375611\n",
      "Accuracy:0.9919\n",
      "\n",
      "2023-08-26 20:58:23 Epoch 20, Training loss 0.00018839034567259374\n",
      "Accuracy:0.9935\n",
      "\n",
      "2023-08-26 21:01:45 Epoch 30, Training loss 7.161747659345401e-05\n",
      "Accuracy:0.9941\n",
      "\n",
      "2023-08-26 21:05:05 Epoch 40, Training loss 1.1692264663161806e-05\n",
      "Accuracy:0.994\n",
      "\n",
      "2023-08-26 21:08:27 Epoch 50, Training loss 5.7541708288787225e-06\n",
      "Accuracy:0.9942\n",
      "\n",
      "2023-08-26 21:11:48 Epoch 60, Training loss 3.9080913309766514e-06\n",
      "Accuracy:0.9942\n",
      "\n",
      "2023-08-26 21:15:10 Epoch 70, Training loss 2.949779945847441e-06\n",
      "Accuracy:0.9943\n",
      "\n",
      "2023-08-26 21:18:30 Epoch 80, Training loss 2.3545971178618657e-06\n",
      "Accuracy:0.9942\n",
      "\n",
      "2023-08-26 21:21:52 Epoch 90, Training loss 1.9614544687796944e-06\n",
      "Accuracy:0.9942\n",
      "\n",
      "2023-08-26 21:25:12 Epoch 100, Training loss 1.6696898435728566e-06\n",
      "Accuracy:0.9943\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 256\n",
    "conv2_out_ch = 128\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "model_7 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_7.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_7.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f224ac83",
   "metadata": {},
   "source": [
    "Повышение кол-во карт дало существенной прирост по сравнению с предыдущими моделями - думмаю нужно двигаться к их увеличению и увеличению слоев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b449d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-27 05:30:01 Epoch 1, Training loss 1.9397288075705836\n",
      "Accuracy:0.7197\n",
      "\n",
      "2023-08-27 05:40:19 Epoch 10, Training loss 0.010710838393968202\n",
      "Accuracy:0.9925\n",
      "\n",
      "2023-08-27 05:51:41 Epoch 20, Training loss 0.0010164938588836932\n",
      "Accuracy:0.9927\n",
      "\n",
      "2023-08-27 06:03:02 Epoch 30, Training loss 2.1671703263522808e-05\n",
      "Accuracy:0.9941\n",
      "\n",
      "2023-08-27 06:14:25 Epoch 40, Training loss 8.166129207145503e-06\n",
      "Accuracy:0.9941\n",
      "\n",
      "2023-08-27 06:25:58 Epoch 50, Training loss 4.527776637581029e-06\n",
      "Accuracy:0.9942\n",
      "\n",
      "2023-08-27 06:37:27 Epoch 60, Training loss 3.028879287861703e-06\n",
      "Accuracy:0.9942\n",
      "\n",
      "2023-08-27 06:48:52 Epoch 70, Training loss 2.240915111124476e-06\n",
      "Accuracy:0.9941\n",
      "\n",
      "2023-08-27 07:00:21 Epoch 80, Training loss 1.7642279231504317e-06\n",
      "Accuracy:0.994\n",
      "\n",
      "2023-08-27 07:11:46 Epoch 90, Training loss 1.4458317705995075e-06\n",
      "Accuracy:0.994\n",
      "\n",
      "2023-08-27 07:23:14 Epoch 100, Training loss 1.2216452796530528e-06\n",
      "Accuracy:0.994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 1024\n",
    "conv2_out_ch = 512\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "model_7 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 2048),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_7.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_7.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93bcea",
   "metadata": {},
   "source": [
    "0 результатов. Увеличение размера и кол-ва слоев не помогло "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf063497",
   "metadata": {},
   "source": [
    "Сначала задам равные conv1_out_ch = conv2_out_ch = 256, вместо conv2_out_ch = 128\n",
    "\n",
    "Я тут тестил разные модели, которые удалил тк особых результатов от них нет и выводово особо тоже. Но вот вопросик один появился:  \n",
    "Всегда ли последующие слои должны быть меньше, можно ли сделать их такими или больше? Как я понимаю начальные слои собирают низкоуровневую информацию, а последующие более высокоуровневую, и логично что низкору будет меньше, чем высокоур. Если привести аналогию то распознование лица - сначала распознование прямых и крывых линий - расспознование, условно, частей лица нос и тд - потом уже распознавание лица. Логично что кривых и прямых линий будет сильно больше, чем лиц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09843f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-27 11:55:33 Epoch 1, Training loss 0.9384992567679362\n",
      "Accuracy:0.9753\n",
      "\n",
      "2023-08-27 11:59:04 Epoch 10, Training loss 0.008409673257561864\n",
      "Accuracy:0.9919\n",
      "\n",
      "2023-08-27 12:02:58 Epoch 20, Training loss 0.00012922786370984322\n",
      "Accuracy:0.9934\n",
      "\n",
      "2023-08-27 12:06:51 Epoch 30, Training loss 1.4310439179962322e-05\n",
      "Accuracy:0.9934\n",
      "\n",
      "2023-08-27 12:10:43 Epoch 40, Training loss 7.230054621017709e-06\n",
      "Accuracy:0.9935\n",
      "\n",
      "2023-08-27 12:14:37 Epoch 50, Training loss 4.759481349186521e-06\n",
      "Accuracy:0.9936\n",
      "\n",
      "2023-08-27 12:18:32 Epoch 60, Training loss 3.5251385771345635e-06\n",
      "Accuracy:0.9937\n",
      "\n",
      "2023-08-27 12:22:26 Epoch 70, Training loss 2.789405340655103e-06\n",
      "Accuracy:0.9937\n",
      "\n",
      "2023-08-27 12:26:23 Epoch 80, Training loss 2.2979096850921944e-06\n",
      "Accuracy:0.9937\n",
      "\n",
      "2023-08-27 12:30:19 Epoch 90, Training loss 1.9450729714554578e-06\n",
      "Accuracy:0.9936\n",
      "\n",
      "2023-08-27 12:34:16 Epoch 100, Training loss 1.6837195106209619e-06\n",
      "Accuracy:0.9936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 256\n",
    "conv2_out_ch = 128\n",
    "\n",
    "kernel_size = 5\n",
    "\n",
    "model_7 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_7.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_7.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30118b65",
   "metadata": {},
   "source": [
    "Тут отдельно хочу поэксперементировать с размером батча\n",
    "\n",
    "тут я нашел небольшой косяк в коде и нужно функцию переопределить\n",
    "\n",
    "полохая была идея..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f7306",
   "metadata": {},
   "source": [
    "почему то до этого момента более глубокии слои сверток были более узкими, попробую это поменять\n",
    "\n",
    "https://tproger.ru/translations/neural-network-zoo-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0315682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-27 16:09:42 Epoch 1, Training loss 0.8798398897270245\n",
      "Accuracy:0.9703\n",
      "\n",
      "2023-08-27 16:12:24 Epoch 10, Training loss 0.006422967504411195\n",
      "Accuracy:0.9904\n",
      "\n",
      "2023-08-27 16:15:29 Epoch 20, Training loss 0.00017778021417556397\n",
      "Accuracy:0.9935\n",
      "\n",
      "2023-08-27 16:18:35 Epoch 30, Training loss 1.3667002999856187e-05\n",
      "Accuracy:0.9931\n",
      "\n",
      "2023-08-27 16:21:41 Epoch 40, Training loss 7.153756191634156e-06\n",
      "Accuracy:0.9933\n",
      "\n",
      "2023-08-27 16:24:44 Epoch 50, Training loss 4.776609574080017e-06\n",
      "Accuracy:0.9933\n",
      "\n",
      "2023-08-27 16:27:44 Epoch 60, Training loss 3.5685762851600713e-06\n",
      "Accuracy:0.9933\n",
      "\n",
      "2023-08-27 16:30:43 Epoch 70, Training loss 2.8460352039128354e-06\n",
      "Accuracy:0.9933\n",
      "\n",
      "2023-08-27 16:33:49 Epoch 80, Training loss 2.3438197777101854e-06\n",
      "Accuracy:0.9934\n",
      "\n",
      "2023-08-27 16:36:53 Epoch 90, Training loss 1.9898233368847297e-06\n",
      "Accuracy:0.9934\n",
      "\n",
      "2023-08-27 16:39:57 Epoch 100, Training loss 1.7266995217499767e-06\n",
      "Accuracy:0.9934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 128\n",
    "conv2_out_ch = 256\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "model_7 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "\n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_7.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_7.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329e463d",
   "metadata": {},
   "source": [
    "# Skip connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e601b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-27 16:40:15 Epoch 1, Training loss 0.8309730087686926\n",
      "Accuracy:0.9768\n",
      "\n",
      "2023-08-27 16:42:39 Epoch 10, Training loss 0.011157308909804842\n",
      "Accuracy:0.9929\n",
      "\n",
      "2023-08-27 16:45:18 Epoch 20, Training loss 0.0024344149873257765\n",
      "Accuracy:0.9934\n",
      "\n",
      "2023-08-27 16:47:57 Epoch 30, Training loss 0.00011417159196535736\n",
      "Accuracy:0.9937\n",
      "\n",
      "2023-08-27 16:50:36 Epoch 40, Training loss 2.4062219069191723e-05\n",
      "Accuracy:0.9939\n",
      "\n",
      "2023-08-27 16:53:14 Epoch 50, Training loss 1.0882997679414942e-05\n",
      "Accuracy:0.9939\n",
      "\n",
      "2023-08-27 16:55:54 Epoch 60, Training loss 6.636198981263162e-06\n",
      "Accuracy:0.9939\n",
      "\n",
      "2023-08-27 16:58:32 Epoch 70, Training loss 4.677201194928006e-06\n",
      "Accuracy:0.9939\n",
      "\n",
      "2023-08-27 17:01:12 Epoch 80, Training loss 3.634082538007987e-06\n",
      "Accuracy:0.9939\n",
      "\n",
      "2023-08-27 17:03:51 Epoch 90, Training loss 2.9641801400934297e-06\n",
      "Accuracy:0.9939\n",
      "\n",
      "2023-08-27 17:06:29 Epoch 100, Training loss 2.4927990121787315e-06\n",
      "Accuracy:0.9939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 128\n",
    "#conv2_out_ch = 256\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "class SkipConv(nn.Module):\n",
    "    def __init__(self, n_ch = 128, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_ch\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2d = nn.Conv2d(n_ch, n_ch, kernel_size=kernel_size, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv2d(x))\n",
    "        return out + x\n",
    "\n",
    "model_7 = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # skip con start\n",
    "            \n",
    "            SkipConv(n_ch= conv1_out_ch, kernel_size=kernel_size),\n",
    "            #x\n",
    "            #out = nn.Conv2d(conv1_out_ch, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            #out= nn.ReLU(),\n",
    "            #x + out\n",
    "\n",
    "\n",
    "            # skip connection end\n",
    "            \n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv1_out_ch * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "model_7.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(model_7.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, model_7, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6aa1c1",
   "metadata": {},
   "source": [
    "Добавлю еще один сверточный слой "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47a728b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-27 17:14:52 Epoch 1, Training loss 0.9711052795161189\n",
      "Accuracy:0.9698\n",
      "\n",
      "2023-08-27 17:18:24 Epoch 10, Training loss 0.009495700359587382\n",
      "Accuracy:0.9908\n",
      "\n",
      "2023-08-27 17:22:19 Epoch 20, Training loss 0.004351191368040678\n",
      "Accuracy:0.9924\n",
      "\n",
      "2023-08-27 17:26:13 Epoch 30, Training loss 9.085706549284093e-05\n",
      "Accuracy:0.9925\n",
      "\n",
      "2023-08-27 17:30:08 Epoch 40, Training loss 7.807668673342872e-06\n",
      "Accuracy:0.9923\n",
      "\n",
      "2023-08-27 17:34:03 Epoch 50, Training loss 4.212952766970865e-06\n",
      "Accuracy:0.9923\n",
      "\n",
      "2023-08-27 17:37:56 Epoch 60, Training loss 2.8453038275320317e-06\n",
      "Accuracy:0.9924\n",
      "\n",
      "2023-08-27 17:41:50 Epoch 70, Training loss 2.1385047320655724e-06\n",
      "Accuracy:0.9924\n",
      "\n",
      "2023-08-27 17:45:43 Epoch 80, Training loss 1.7017288948364649e-06\n",
      "Accuracy:0.9924\n",
      "\n",
      "2023-08-27 17:49:35 Epoch 90, Training loss 1.4102514238833308e-06\n",
      "Accuracy:0.9924\n",
      "\n",
      "2023-08-27 17:53:27 Epoch 100, Training loss 1.200461228102091e-06\n",
      "Accuracy:0.9924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 256\n",
    "conv2_out_ch = 128\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "class SkipConv(nn.Module):\n",
    "    def __init__(self, n_ch = 256, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_ch\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2d = nn.Conv2d(n_ch, n_ch, kernel_size=kernel_size, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv2d(x))\n",
    "        return out + x\n",
    "\n",
    "final_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # skip con start\n",
    "            \n",
    "            SkipConv(n_ch= conv1_out_ch, kernel_size=kernel_size),\n",
    "            #x\n",
    "            #out = nn.Conv2d(conv1_out_ch, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            #out= nn.ReLU(),\n",
    "            #x + out\n",
    "\n",
    "\n",
    "            # skip connection end\n",
    "            \n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            #3й сверточный слой \n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            # а что если не добавить макс пулинг\n",
    "\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(final_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, final_model, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a443bfa5",
   "metadata": {},
   "source": [
    "а теперь добавим пулинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e83a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-27 17:53:52 Epoch 1, Training loss 1.2523899675011316\n",
      "Accuracy:0.9677\n",
      "\n",
      "2023-08-27 17:57:21 Epoch 10, Training loss 0.009862853464215025\n",
      "Accuracy:0.9911\n",
      "\n",
      "2023-08-27 18:01:12 Epoch 20, Training loss 0.003112531796822839\n",
      "Accuracy:0.992\n",
      "\n",
      "2023-08-27 18:05:02 Epoch 30, Training loss 7.967801176575928e-05\n",
      "Accuracy:0.9938\n",
      "\n",
      "2023-08-27 18:08:52 Epoch 40, Training loss 1.947875563521613e-05\n",
      "Accuracy:0.994\n",
      "\n",
      "2023-08-27 18:12:43 Epoch 50, Training loss 6.99052802631778e-06\n",
      "Accuracy:0.9939\n",
      "\n",
      "2023-08-27 18:16:33 Epoch 60, Training loss 4.413624695834941e-06\n",
      "Accuracy:0.9936\n",
      "\n",
      "2023-08-27 18:20:20 Epoch 70, Training loss 3.229604602775565e-06\n",
      "Accuracy:0.9936\n",
      "\n",
      "2023-08-27 18:24:09 Epoch 80, Training loss 2.533732556081434e-06\n",
      "Accuracy:0.9936\n",
      "\n",
      "2023-08-27 18:28:09 Epoch 90, Training loss 2.0699962397127995e-06\n",
      "Accuracy:0.9936\n",
      "\n",
      "2023-08-27 18:32:08 Epoch 100, Training loss 1.748571074323172e-06\n",
      "Accuracy:0.9936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conv1_out_ch = 256\n",
    "conv2_out_ch = 128\n",
    "\n",
    "kernel_size = 3\n",
    "\n",
    "class SkipConv(nn.Module):\n",
    "    def __init__(self, n_ch = 256, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_ch\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2d = nn.Conv2d(n_ch, n_ch, kernel_size=kernel_size, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv2d(x))\n",
    "        return out + x\n",
    "\n",
    "final_model = nn.Sequential(\n",
    "            nn.Conv2d(1, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            # skip con start\n",
    "            \n",
    "            SkipConv(n_ch= conv1_out_ch, kernel_size=kernel_size),\n",
    "            #x\n",
    "            #out = nn.Conv2d(conv1_out_ch, conv1_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            #out= nn.ReLU(),\n",
    "            #x + out\n",
    "\n",
    "\n",
    "            # skip connection end\n",
    "            \n",
    "\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            #3й сверточный слой \n",
    "            nn.Conv2d(conv1_out_ch, conv2_out_ch, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(conv2_out_ch * 3 * 3, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Dropout(p=0.4),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Linear(32, 10))\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "learning_rate = 1e-1\n",
    "\n",
    "optimizer = optim.SGD(final_model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() \n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "training_loop(n_epochs, optimizer, final_model, loss_fn, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca98993",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
